{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the HW grading is done in a semi-automatic manner, please adhere to the following naming format for your submission.\n",
    "Each group of students (mostly pairs, with some approved exceptions) should submit a Jupyter notebook (.ipynb file and not a .zip file) whose name is the underscored-separated id list of all the submitters. \n",
    "For example, for two submitters, the naming format is: id1_id2.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Download the \"Boston1.csv\" database, and explore the data. Explanation about the dataset can be found here: http://www.clemson.edu/economics/faculty/wilson/R-tutorial/analyzing_data.html\n",
    "\n",
    "Find the columns with missing values and filter them out of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:  ['misData']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Boston1.csv')\n",
    "cols_with_missing = [col for col in df.columns if df[col].isnull().any()]\n",
    "print('Columns with missing values: ', cols_with_missing)\n",
    "\n",
    "# Filter out the columns with missing values\n",
    "df_filtered = df.drop(columns=cols_with_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Divide the filtered data randomly into a train set (70% of the data) and test set (30% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df_filtered, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't done this previously, install the scikit-learn package for python.\n",
    "\n",
    "a) On the train set, run a linear regression model as follows:\n",
    "Divide the training set into explanatory variables (the X matrix with which we'll try to make a prediction) and a target variable (y, the value which we'll try to predict). Use the 'medv' attribute as the target variable y and the rest of the features as the X matrix. Run a linear regression model on those sets, and print the regression coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.31554143e-01  3.56062850e-02  4.76542127e-02  3.14666736e+00\n",
      " -1.51275836e+01  4.07563132e+00 -1.10715557e-02 -1.38369791e+00\n",
      "  2.41853708e-01 -8.74151558e-03 -9.10127712e-01  1.18064380e-02\n",
      " -5.46257067e-01 -5.51868467e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "mdl     = LinearRegression()\n",
    "train_x = train_df.drop(['medv'],axis=1)\n",
    "train_y = train_df['medv']\n",
    "test_x  = test_df.drop(['medv'],axis=1)\n",
    "test_y  = test_df['medv']\n",
    "\n",
    "mdl.fit(train_x, train_y)\n",
    "m = mdl.coef_\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use the linear regression model to predict the values of the test set's 'medv' column, based on the test set's other attributes. Print the Mean Squared Error of the model on the train set and on the test set.\n",
    "Usually, the MSE on the train set would be lower than the MSE on the test set, since the model parameters are optimized with respect to the train set. Must this always be the case? Can you think of a few examples for when this might not be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of the train set:\n",
      " 22.52265349764171\n",
      "MSE of the test set:\n",
      " 21.774657824501453\n",
      "1) Noise in the Training Data: If the training data has a significant amount of noise or outliers that the model tries to fit, the training MSE might be high. On the other hand, if the test data is cleaner without such noise or outliers, the test MSE might be lower.\n",
      "2) Regularization: If a model is trained with strong regularization, it may be underfit to the training set and thus have a higher training error.\n",
      "3) Small Test Set: If the test set is relatively small, it might just by chance contain easier examples that the model can predict more accurately, resulting in a lower test MSE.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "print(\"MSE of the train set:\\n\", mse(mdl.predict(train_x), train_y))\n",
    "print(\"MSE of the test set:\\n\",mse(mdl.predict(test_x), test_y))\n",
    "print(\"1) Noise in the Training Data: If the training data has a significant amount of noise or outliers that the model tries to fit, the training MSE might be high. On the other hand, if the test data is cleaner without such noise or outliers, the test MSE might be lower.\\n2) Regularization: If a model is trained with strong regularization, it may be underfit to the training set and thus have a higher training error.\\n3) Small Test Set: If the test set is relatively small, it might just by chance contain easier examples that the model can predict more accurately, resulting in a lower test MSE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Add some noise (with mean=0, std=1) to the test set's y, and predict it again. What happened to the MSE? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of the test set after adding some noise:\n",
      " 21.85764233190693\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Add some noise\n",
    "noise = np.random.normal(0, 1, len(test_y.index))\n",
    "noised_test_y = test_y + noise\n",
    "print(\"MSE of the test set after adding some noise:\\n\",mse(mdl.predict(test_x), noised_test_y))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Create a Recursive feature elimination model, with a linear regression estimator, that selects half of the original number of features. Hint: Check the feature_selection module in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Thanks for your hint\n",
    "rfe = RFE(mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use the feature elimination model on the full database (after filtering columns with missing values, before partitioning into train/test). Print the features that were selected. Remember that we separate the 'medv' attribute to be our y, while the rest of the attributes in the dataset serve as features to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features that were selected:\n",
      "['chas' 'nox' 'rm' 'dis' 'ptratio' 'lstat' 'randCol']\n"
     ]
    }
   ],
   "source": [
    "X = df_filtered.drop(['medv'],axis=1)\n",
    "y = df_filtered['medv']\n",
    "selected_model = rfe.fit(X,y)\n",
    "print(\"The features that were selected:\")\n",
    "print(X.columns[selected_model.support_].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We'd like to find out the optimal number of features. Create feature elimination models (with linear regression estimators) for every number of features between 1 and n (where n = all the original features, 'medv' excluded). For each number of features, run a linear regression as in Question 2, only on the selected features, in order to predict 'medv'. Print the Mean Sqaured Error for each number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1 selected features the MSE is: 69.0042883554067\n",
      "For 2 selected features the MSE is: 39.21811674276104\n",
      "For 3 selected features the MSE is: 37.518269887684575\n",
      "For 4 selected features the MSE is: 32.446947181499546\n",
      "For 5 selected features the MSE is: 30.929546079491566\n",
      "For 6 selected features the MSE is: 23.99421489307857\n",
      "For 7 selected features the MSE is: 23.988679676026152\n",
      "For 8 selected features the MSE is: 23.873140175553736\n",
      "For 9 selected features the MSE is: 23.34492967422981\n",
      "For 10 selected features the MSE is: 23.255560445416666\n",
      "For 11 selected features the MSE is: 22.930361610394424\n",
      "For 12 selected features the MSE is: 22.42502862441528\n",
      "For 13 selected features the MSE is: 21.880860827853045\n",
      "For 14 selected features the MSE is: 21.880721616729907\n"
     ]
    }
   ],
   "source": [
    "n=X.shape[1]\n",
    "mses = list()\n",
    "for i in range (1,n+1):\n",
    "    rfe_i = RFE(estimator=mdl, n_features_to_select=i)\n",
    "    selected_model = rfe_i.fit(X,y)\n",
    "    selected_features = X.columns[selected_model.support_].values\n",
    "    selected_X = X.reindex(columns = selected_features)\n",
    "    mdl.fit(selected_X,y)\n",
    "    mse_i = mse(y, mdl.predict(selected_X))\n",
    "    print(\"For\",i,\"selected features the MSE is:\",mse_i)\n",
    "    mses.append(mse_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Conclude the optimal number of features for this task. Think about the cost of adding for data vs the benefit of a more accurate prediction. Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"From the results, we can point for 2 massive MSE drops.\\nOne from moving from 1 to 2 features, and another from moving from 5 to 6 features.\")\n",
    "print(\"In this case, we would conclude that the optimal number of features for this task is 6. Due to the cost of adding for data vs the benefit of a more accurate prediction. Because we could pick all the features (as seen have the lowest MSE) but the cost would be huge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a cross-validation of the linear regression on the train set with K=5. Print the CV scores for each repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74512757 0.51729534 0.75283635 0.76628987 0.6464534 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(mdl, train_x, train_y, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
